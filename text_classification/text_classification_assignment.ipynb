{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DeYXbqK-rRW"
      },
      "source": [
        "## Classifying text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MejilF82-rRZ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV as gs\n",
        "import sklearn.feature_extraction.text as text\n",
        "import sklearn.naive_bayes as nb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqYT-QRJEhN5"
      },
      "source": [
        "We turn to applying machine learning classification methods to text. There are\n",
        "no new principles at stake.  In principle, everything is the same as it was for\n",
        "learning how to classify irises.\n",
        "\n",
        "1.  We need to find labeled data; each of the exemplars in the data should be represented with a fixed set of features.  \n",
        "2. We need to split our data and training and test data.  \n",
        "3. We need to train learner on the training data and evaluate it (test it) it on the test data.\n",
        "\n",
        "The problem is that text data is not in a form  that is compatible with\n",
        "what we have learned about classifiers.  The text must be put in a suitable\n",
        "form before a linear model; can be trained on it.\n",
        "\n",
        "**Training**\n",
        "\n",
        "1.  Labeled data must be loaded (into Python).  It should be a sequence of documents T accompanied by a sequence of labels L.\n",
        "2.  Split T and L into training and test groups, yielding T1 and T2; as well as and L1 and L2.\n",
        "2.  Train or a **feature model** on the training data T1 (or in scikit learn terminology **fit** the model **to** the training data).  The feature model inputs the text sequence and outputs a **term-document** matrix suitable for training a linear classifier.  The feature model is called a **vectorizer**\n",
        "(because it turns a document into a vector, a column of numbers).\n",
        "3.  Using the trained vectorizer, transform T1 into a term document matrix M1.\n",
        "4.  Train a linear model $\\mu$ on M1 and L1.\n",
        "\n",
        "**Evaluation**\n",
        "\n",
        "1.  Transform the test data T2 into a term document matrix M2 using the vectorizer fit during step 2 of training;  in particular this means if there are words in the T2 data that were never seen during training, they are ignored in building M2.\n",
        "2.  Use $\\mu$  to classify the texts represented in M2; that is produce a set of predicted labels P2.\n",
        "3.  Compare the actual labels L2 with the predicted labels P2 using standard evaluation metrics such as precision, accuracy, and recall.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qildTjvw-rRb"
      },
      "source": [
        "## Review the steps with insult detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28jZSTW_-rRb"
      },
      "source": [
        "We looked at the insult detection data in  the text classification notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJaUSzpQEhN7"
      },
      "source": [
        "### Training step 1: Loading the data\n",
        "\n",
        "Let's load the CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "MtGQlB1q-rRc"
      },
      "outputs": [],
      "source": [
        "import os.path\n",
        "site = 'https://raw.githubusercontent.com/gawron/python-for-social-science/master/'\\\n",
        "'text_classification/'\n",
        "#site = 'https://gawron.sdsu.edu/python_for_ss/course_core/book_draft/_static/'\n",
        "df = pd.read_csv(os.path.join(site,\"troll.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3ncLUYx-rRc"
      },
      "source": [
        "Each row is a comment  taken from a blog or online forum. There are three columns: whether the comment is insulting (1) or not (0), the date, and the comment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pFeNaW-m-rRd",
        "outputId": "f26e89a0-8133-41cb-bfbd-ede1b9b01de8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Insult             Date  \\\n",
              "3942       1  20120502172717Z   \n",
              "3943       0  20120528164814Z   \n",
              "3944       0  20120620142813Z   \n",
              "3945       0  20120528205648Z   \n",
              "3946       0  20120515200734Z   \n",
              "\n",
              "                                                Comment  \n",
              "3942  \"you are both morons and that is never happening\"  \n",
              "3943  \"Many toolbars include spell check, like Yahoo...  \n",
              "3944  \"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...  \n",
              "3945  \"How about Felix? He is sure turning into one ...  \n",
              "3946  \"You're all upset, defending this hipster band...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0540fc10-c2d9-4ffa-bb06-698ec1d325a3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Insult</th>\n",
              "      <th>Date</th>\n",
              "      <th>Comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3942</th>\n",
              "      <td>1</td>\n",
              "      <td>20120502172717Z</td>\n",
              "      <td>\"you are both morons and that is never happening\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3943</th>\n",
              "      <td>0</td>\n",
              "      <td>20120528164814Z</td>\n",
              "      <td>\"Many toolbars include spell check, like Yahoo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3944</th>\n",
              "      <td>0</td>\n",
              "      <td>20120620142813Z</td>\n",
              "      <td>\"@LambeauOrWrigley\\xa0\\xa0@K.Moss\\xa0\\nSioux F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3945</th>\n",
              "      <td>0</td>\n",
              "      <td>20120528205648Z</td>\n",
              "      <td>\"How about Felix? He is sure turning into one ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3946</th>\n",
              "      <td>0</td>\n",
              "      <td>20120515200734Z</td>\n",
              "      <td>\"You're all upset, defending this hipster band...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0540fc10-c2d9-4ffa-bb06-698ec1d325a3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0540fc10-c2d9-4ffa-bb06-698ec1d325a3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0540fc10-c2d9-4ffa-bb06-698ec1d325a3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f0e31efa-6292-4950-8280-4016165f1f74\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f0e31efa-6292-4950-8280-4016165f1f74')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f0e31efa-6292-4950-8280-4016165f1f74 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibxc3U3K-rRh"
      },
      "source": [
        "Now we define the text sequences $\\mathbf{T}$ and the label sequence  $\\mathbf{L}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "wty7pj42-rRh"
      },
      "outputs": [],
      "source": [
        "T = df['Comment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-8xJstOo-rRi"
      },
      "outputs": [],
      "source": [
        "L = df['Insult']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AC0hb0XFEhOC"
      },
      "source": [
        "### Step 2 Split the data and labels into training and test groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "THfTSC43EhOC"
      },
      "outputs": [],
      "source": [
        "T1, T2, L1, L2 = train_test_split(T,L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpHs6Xy8EhOD"
      },
      "source": [
        "### Step 3 and 4:  Fit the feature model (vectorizer) to the training data and Transform  it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "H-fPVjYV-rRl"
      },
      "outputs": [],
      "source": [
        "tf = text.TfidfVectorizer()\n",
        "# Scikit learn has one function that does both fitting and transforming.\n",
        "# M1 is the transformed data\n",
        "# tf is the trained feature model (which will be used to transform the test data)\n",
        "M1 = tf.fit_transform(T1)#.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0wp6RbS-rRo"
      },
      "source": [
        "### Step 5 Training the classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88mG_1C0-rRo"
      },
      "source": [
        "Now, we are going to train a classifier as usual. We first split the data into a train and test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf-3XQDN-rRo"
      },
      "source": [
        "We use a **Bernoulli Naive Bayes classifier**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "uj7vvTS--rRo"
      },
      "outputs": [],
      "source": [
        "# Create classifer\n",
        "bnb =nb.BernoulliNB()\n",
        "#bnb= nb.MultinomialNB()\n",
        "#bnb =nb.GaussianNB()\n",
        "\n",
        "# Fit (train) the classifier  using the training data and labels\n",
        "bnb.fit(M1, L1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7vqzvM-EhOE"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5x47lXrEhOE"
      },
      "source": [
        "Evaluate the classifier, first using accuracy (what `.score()` returns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rewbP2vT-rRp",
        "outputId": "aa18b8b6-559a-4218-d6d9-54f6bab7927b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7882472137791287"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# vectorize the test data using the vectorizer trained on T1\n",
        "# Notice we DONT call .fit_transform() because that would retrain the vectorizer on the test data\n",
        "# We call .transform() using the trained model to transform the new data.\n",
        "# Words not seen during training will be ignored.\n",
        "M2 = tf.transform(T2)#.toarray()\n",
        "# Classify the data using the trained classisifer and report the accuracy\n",
        "bnb.score(M2, L2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p38bf79W-rRp"
      },
      "source": [
        "Now try re-executing steps 2 through 5.  (Just re-execute the cells)  The results should be the same, right?\n",
        "\n",
        "Well, are they?  \n",
        "\n",
        "What happens:  each training test split produces a different set of test data.  Sometimes the test is harder.\n",
        "Sometimes it's easier.  Or looking at it another way:  Sometimes the training data is a better preparation for the test than others.  \n",
        "\n",
        "To get a realistic view of how our classifier is doing we take the average performance on a  number of\n",
        "train/test splits.  This is called **cross validation**.  We return to that point below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50q53Oo2EhOF"
      },
      "source": [
        "#### Using all three evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeSyFOIkEhOF"
      },
      "source": [
        "First let's get more evaluation numbers, in particular precision and recall.  We do\n",
        "that by calling a method that returns the predicted labels P2, so we can compare\n",
        "L2 and P2 using different evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "VQb4h6v4EhOG",
        "outputId": "40d79db1-54aa-48d6-f604-4e2cb4407901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.79 Precision: 0.15 Recall: 0.92\n"
          ]
        }
      ],
      "source": [
        "P2 = bnb.predict(M2)\n",
        "scores = np.array([accuracy_score(P2, L2),\n",
        "                   precision_score(P2, L2),\n",
        "                   recall_score(P2, L2)])\n",
        "print(f'Accuracy: {scores[0]:.2f} Precision: {scores[1]:.2f} Recall: {scores[2]:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YoZsh6tEhOG"
      },
      "source": [
        "We see that the accuracy is a bit misleading.  There is a serious precision problem.\n",
        "\n",
        "What does that mean in the setting of insult detection?  It means the BNB classifier is a little too\n",
        "eager to call something an insult.  When it flags something as an insult, it\n",
        "is right only 14% of the time.\n",
        "\n",
        "Why would that be?  Think about how the model is trained and what its weakness might be.\n",
        "This is what it means to try to interpret or discuss a model's performance.  Zoom\n",
        "in the model's weakness. Talk about where that weakness comes from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-bt36GeMpFW"
      },
      "source": [
        "#### Basic train and test loop\n",
        "\n",
        "How to get the average of a number of runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "zaE0KDK5-rRr"
      },
      "source": [
        "## Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg50G61U-rRr"
      },
      "source": [
        "Read the on line book draft chapter about text classification and and especially\n",
        "about insult detection. Focus on the use of `scikit_learn`, especially the\n",
        "`TfidfVectorizer`.\n",
        "\n",
        "Try two different classifiers on the movie review data, the one used in the textbook, an SVM called\n",
        "`LinearSVC`, and  the Bernoulli Naive Bayes model used above. Some points of emphasis;\n",
        "\n",
        "1.  Be sure to get the average of at runs  least 10 runs for **both** classifiers.\n",
        "2.  Be sure to get average accuracy, precision, and recall for both classifiers on those multiple runs. You will probably find `split_fit_and_eval` defined above useful, but you will need to modify it.\n",
        "3.  For your first discussion post turn in the new code you wrote, including the code that labels and shuffles the data (discussed further below).  If you have to do a new import, show that. If you have to rewrite `split_fit_and_eval`, turn in the new version.  Also show the output, which should be a single line giving the accuracy, prcision, and recall.\n",
        "4.  Discuss which classifier does better.  Discuss which metric the best classifier does the worst at and speculate as to why (this will require reviewing the definitions of precision and recall and thinking about what they mean in a movie review setting).\n",
        "5. Using the SVM classifier and training on **all** the data find the 50 most important Positive features for the Movie Reviews Data.  They should differ significantly from the most important features in Insult Detection. The function `print_topn` (from the Insult Detection Notebook) should be of help.\n",
        "6.  Find the 100 most important Negative features for the Movie Reviews Data. Note that the way two-class problems work with SVMs there is only one set of weights to look at, so it won't work to pass more than one class name to the `class_labels` parameter of `print_top_n` (it would work with a NaiveBayes classifier).  In particular: you need the **lowest** weighted features if you want to look at the more fun word set that best characterized bad reviews. You will have to modify `print_topn` (from the Insult Detection Notebook) to do that. Try to do so in such a way that with one set of parameters it prints the most positive words, and with another, it prints the most negative words. You may notice the names of a few actors appearing in this feature set. Try not to laugh as the meaning of this dawns on you.  For an extra bit of approval from your instructor, while you're at it, modify it so that it returns a list of words in addition to printing them.  You should probably change the name of the function to `get_topn` if you succeed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_wazBGWTsS4"
      },
      "source": [
        "#### Help with getting the movie reviews data.\n",
        "\n",
        "Execute the next two cells to get the movie review data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvlkaI1x-rRr",
        "outputId": "f49cf5eb-160d-4f89-e613-4715ec54087a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "8nQdcr4aH_dP"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import movie_reviews as mr\n",
        "\n",
        "def get_file_strings (corpus, file_ids):\n",
        "    return [corpus.raw(file_id) for file_id in file_ids]\n",
        "\n",
        "data = dict(pos = mr.fileids('pos'),\n",
        "            neg = mr.fileids('neg'))\n",
        "\n",
        "pos_file_ids = data['pos']\n",
        "neg_file_ids = data['neg']\n",
        "\n",
        "# Get all the positive and negative reviews.\n",
        "pos_file_reviews = get_file_strings (mr, pos_file_ids)\n",
        "neg_file_reviews = get_file_strings (mr, neg_file_ids)\n",
        "\n",
        " # Organize reviews as tuples of (document, label)\n",
        "pos_documents = [(review, 'pos') for review in pos_file_reviews]\n",
        "neg_documents = [(review, 'neg') for review in neg_file_reviews]\n",
        "\n",
        "    # Combine positive and negative reviews\n",
        "all_documents = pos_documents + neg_documents\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PY6_uHMJvqC"
      },
      "source": [
        "Each review is a string.  In principle, a list of strings like `pos_file_reviews`  can be passed to `text.TfidfVectorizer()` via the `fit_transform` method to train a vectorizer for machine learning.\n",
        "You could code that up.\n",
        "\n",
        "What you'd really like to do is use `split_fit_and_eval`, defined above, which does a lot of the work for you.\n",
        "\n",
        "But hold on. You have a coding problem. You don't have  a sequence of documents and labels.  Instead you have\n",
        "one sequence of positive documents  and another sequence of negative documents.  \n",
        "\n",
        "So you will need to turn those two sequences into a sequence of documents and a sequence of labels\n",
        "because that's what `split_fit_and_eval` wants.  You also want the doc sequence\n",
        "to contain a random mixture of positive and negative documents, because some machine\n",
        "learning algorithms are sensitive to the order in which training data is presented to\n",
        "them.\n",
        "\n",
        "The next cell does **not** do that for you.  But it illustrates an approach using\n",
        "two sets of English letters in place of two sets of English documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCdgmoToRV8z",
        "outputId": "9b332d90-f06e-4c80-bc5c-f2aac14e332d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abcdefghijklm\n",
            "nopqrstuvwxyz\n",
            "('p', 'r', 't', 'j', 'u', 'k', 'c', 'o', 'w', 's', 'v', 'x', 'n', 'g', 'l', 'd', 'z', 'a', 'b', 'i', 'q', 'e', 'f', 'm', 'y', 'h')\n",
            "('l', 'l', 'l', 'f', 'l', 'f', 'f', 'l', 'l', 'l', 'l', 'l', 'l', 'f', 'f', 'f', 'l', 'f', 'f', 'f', 'l', 'f', 'f', 'f', 'l', 'f')\n"
          ]
        }
      ],
      "source": [
        "# Lets work on letters instead of documents\n",
        "# There are 2 classes, letters from the first half of the\n",
        "# alphabet ('f') and letters frmm the last half ('l')\n",
        "\n",
        "from random import shuffle\n",
        "from string import ascii_lowercase\n",
        "\n",
        "#Class 1 of the letters: the f_lets\n",
        "f_lets = ascii_lowercase[:13]\n",
        "print(f_lets)\n",
        "#Class2 of the letters: the l_lets\n",
        "l_lets = ascii_lowercase[13:]\n",
        "print(l_lets)\n",
        "\n",
        "# Now get pairs of letters and labels\n",
        "f_pairs = [(let,'f') for let in f_lets]\n",
        "l_pairs = [(let,'l') for let in l_lets]\n",
        "\n",
        "###########  Shuffling  ###########################\n",
        "# Way too orderly, the classes arent mixed yet.\n",
        "data = f_pairs + l_pairs\n",
        "shuffle(data)\n",
        "###################  Now they're shuffled! ###############\n",
        "\n",
        "# Separate the letters from their labels\n",
        "lets, lbls = zip(*data)\n",
        "print(lets)\n",
        "print(lbls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "gYEgIddQEhOL",
        "outputId": "336e6ad1-0a70-4e6b-c4ba-c0d885267da6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC()"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction import text\n",
        "import sklearn.svm as svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import random\n",
        "from nltk.corpus import movie_reviews as mr\n",
        "\n",
        "def get_file_strings(corpus, file_ids):\n",
        "    return [corpus.raw(file_id) for file_id in file_ids]\n",
        "\n",
        "def load_reviews():\n",
        "    data = dict(pos=mr.fileids('pos'), neg=mr.fileids('neg'))\n",
        "\n",
        "    pos_file_ids = data['pos']\n",
        "    neg_file_ids = data['neg']\n",
        "\n",
        "    # Get all the positive and negative reviews.\n",
        "    pos_file_reviews = get_file_strings(mr, pos_file_ids)\n",
        "    neg_file_reviews = get_file_strings(mr, neg_file_ids)\n",
        "\n",
        "    # Organize reviews as tuples of (document, label)\n",
        "    pos_documents = [(review, 'pos') for review in pos_file_reviews]\n",
        "    neg_documents = [(review, 'neg') for review in neg_file_reviews]\n",
        "\n",
        "    # Combine positive and negative reviews\n",
        "    all_documents = pos_documents + neg_documents\n",
        "\n",
        "    return all_documents\n",
        "\n",
        "# Load reviews as (document, label) tuples\n",
        "documents = load_reviews()\n",
        "\n",
        "def load_and_shuffle_reviews():\n",
        "    documents = load_reviews()\n",
        "    random.shuffle(documents)\n",
        "    return zip(*documents)\n",
        "\n",
        "# Load and shuffle reviews\n",
        "X, y = load_and_shuffle_reviews()\n",
        "\n",
        "# Split the data into training and testing sets (you can adjust the test_size as needed)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TfidfVectorizer and transform the training data\n",
        "tf = TfidfVectorizer()\n",
        "X_train_tfidf = tf.fit_transform(X_train)\n",
        "\n",
        "# Train a LinearSVC classifier\n",
        "est = svm.LinearSVC()\n",
        "est.fit(X_train_tfidf, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_top_features(vectorizer, clf, class_label, top_n=10):\n",
        "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "    try:\n",
        "        class_index = np.where(clf.classes_ == class_label)[0][0]\n",
        "    except IndexError:\n",
        "        print(f\"Class {class_label} not found in classifier classes.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        if clf.coef_.ndim == 1:\n",
        "            coefficients = clf.coef_\n",
        "        else:\n",
        "            coefficients = clf.coef_[0]\n",
        "\n",
        "        # Combine feature names with their coefficients\n",
        "        feature_coef_pairs = list(zip(feature_names, coefficients))\n",
        "        # Sort by coefficient values\n",
        "        sorted_feature_coef_pairs = sorted(feature_coef_pairs, key=lambda x: x[1])\n",
        "\n",
        "        # Get the top_n features\n",
        "        top_features = sorted_feature_coef_pairs[-top_n:]\n",
        "        top_feature_names = [feat for feat, _ in top_features]\n",
        "        print(f\"Top {top_n} features for class {class_label}:\\n{', '.join(top_feature_names)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error printing top features: {e}\")\n",
        "        print(f\"Classes: {clf.classes_}\")\n",
        "        print(f\"Coefficients shape: {clf.coef_.shape}\")\n",
        "        print(f\"Class index: {class_index}\")\n",
        "\n",
        "# Print the top 50 features for the positive class\n",
        "if len(est.classes_) == 2 and 'pos' in est.classes_:\n",
        "    print_top_features(tf, est, class_label='pos', top_n=50)\n",
        "else:\n",
        "    print(\"Binary classification detected, but 'pos' class not found in the classifier.\")\n",
        "\n",
        "# Print the top 100 features for the negative class\n",
        "if len(est.classes_) == 2 and 'neg' in est.classes_:\n",
        "    print_top_features(tf, est, class_label='neg', top_n=100)\n",
        "else:\n",
        "    print(\"Binary classification detected, but 'neg' class not found in the classifier.\")\n"
      ],
      "metadata": {
        "id": "b4ywBdhyXQaj",
        "outputId": "d147be05-ebee-4ca1-ee34-faeed481273a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 50 features for class pos:\n",
            "kits, grunted, hartman, bankability, droagon, banisters, flings, terribly, decaying, brake, showiest, ahabs, rossellini, crowns, lyrical, larroquette, trepidation, sputtering, leoni, tiara, 04, sweat, allegory, mongo, blonde, negligent, rounded, grace, negotiating, staple, profit, stanley, alarming, imbibed, threads, recidivist, arachnids, sloppiness, humorously, peasant, tackles, krueger, hy, jaoui, reclaims, eileen, freshener, ferociously, glacier, amalgamations\n",
            "Top 100 features for class neg:\n",
            "carrion, mortally, amiel, shindler, tit, allegorically, risqueness, exasperating, arc, sophia, neice, mael, chucklesome, medak, proclaimed, swoop, mnc, pettiford, educating, graphically, smearing, torrential, fluent, daddy, onanism, discounted, airs, invokes, driving, interdimensional, mitch, _two_, beart, productions, nehru, mai, treated, exit, brushes, prevue, beleiveable, parr, dirty, torrent, ensemble, scratching, penny, enlistees, mosaic, coward, kits, grunted, hartman, bankability, droagon, banisters, flings, terribly, decaying, brake, showiest, ahabs, rossellini, crowns, lyrical, larroquette, trepidation, sputtering, leoni, tiara, 04, sweat, allegory, mongo, blonde, negligent, rounded, grace, negotiating, staple, profit, stanley, alarming, imbibed, threads, recidivist, arachnids, sloppiness, humorously, peasant, tackles, krueger, hy, jaoui, reclaims, eileen, freshener, ferociously, glacier, amalgamations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like the reviewers tend to not like moviews that are too prude and want movies that make them think and feel smart. Eileen shows up on both sides. I see Stanley which I assume is the famous director Stanley Kubric."
      ],
      "metadata": {
        "id": "9xuzi-b8az1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Function to train and evaluate BernoulliNB classifier\n",
        "def train_and_evaluate_bernoulli(X_train, X_test, y_train, y_test):\n",
        "    # Create a TfidfVectorizer instance\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Fit and transform the training data\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "    # Transform the test data using the same vectorizer\n",
        "    X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train BernoulliNB classifier\n",
        "    clf = BernoulliNB()\n",
        "    clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    return accuracy, precision, recall\n",
        "\n",
        "# Repeat the process 10 times\n",
        "num_repeats = 10\n",
        "accuracies_bernoulli = []\n",
        "precisions_bernoulli = []\n",
        "recalls_bernoulli = []\n",
        "\n",
        "for i in range(num_repeats):\n",
        "    # Load and shuffle reviews\n",
        "    X, y = load_and_shuffle_reviews()\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "\n",
        "    # Train and evaluate the BernoulliNB classifier\n",
        "    accuracy_bernoulli, precision_bernoulli, recall_bernoulli = train_and_evaluate_bernoulli(X_train, X_test, y_train, y_test)\n",
        "    accuracies_bernoulli.append(accuracy_bernoulli)\n",
        "    precisions_bernoulli.append(precision_bernoulli)\n",
        "    recalls_bernoulli.append(recall_bernoulli)\n",
        "\n",
        "    print(f'Repeat {i+1}: Accuracy (BernoulliNB) = {accuracy_bernoulli:.2f}, Precision = {precision_bernoulli:.2f}, Recall = {recall_bernoulli:.2f}')\n",
        "\n",
        "# Display the average accuracy, precision, and recall over 10 repeats for BernoulliNB\n",
        "average_accuracy_bernoulli = np.mean(accuracies_bernoulli)\n",
        "average_precision_bernoulli = np.mean(precisions_bernoulli)\n",
        "average_recall_bernoulli = np.mean(recalls_bernoulli)\n",
        "\n",
        "print(f'\\nAverage Accuracy (BernoulliNB) over {num_repeats} repeats: {average_accuracy_bernoulli:.2f}')\n",
        "print(f'Average Precision (BernoulliNB) over {num_repeats} repeats: {average_precision_bernoulli:.2f}')\n",
        "print(f'Average Recall (BernoulliNB) over {num_repeats} repeats: {average_recall_bernoulli:.2f}')\n"
      ],
      "metadata": {
        "id": "eXYQFc3nPa60",
        "outputId": "10504a84-c3ab-46d5-a2b8-1298330f92ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repeat 1: Accuracy (BernoulliNB) = 0.80, Precision = 0.81, Recall = 0.80\n",
            "Repeat 2: Accuracy (BernoulliNB) = 0.76, Precision = 0.78, Recall = 0.76\n",
            "Repeat 3: Accuracy (BernoulliNB) = 0.76, Precision = 0.79, Recall = 0.76\n",
            "Repeat 4: Accuracy (BernoulliNB) = 0.78, Precision = 0.79, Recall = 0.78\n",
            "Repeat 5: Accuracy (BernoulliNB) = 0.79, Precision = 0.82, Recall = 0.79\n",
            "Repeat 6: Accuracy (BernoulliNB) = 0.79, Precision = 0.80, Recall = 0.79\n",
            "Repeat 7: Accuracy (BernoulliNB) = 0.80, Precision = 0.81, Recall = 0.80\n",
            "Repeat 8: Accuracy (BernoulliNB) = 0.81, Precision = 0.82, Recall = 0.81\n",
            "Repeat 9: Accuracy (BernoulliNB) = 0.80, Precision = 0.82, Recall = 0.80\n",
            "Repeat 10: Accuracy (BernoulliNB) = 0.82, Precision = 0.83, Recall = 0.82\n",
            "\n",
            "Average Accuracy (BernoulliNB) over 10 repeats: 0.79\n",
            "Average Precision (BernoulliNB) over 10 repeats: 0.81\n",
            "Average Recall (BernoulliNB) over 10 repeats: 0.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Function to train and evaluate SVC classifier\n",
        "def train_and_evaluate_svc(X_train, X_test, y_train, y_test):\n",
        "    # Create a TfidfVectorizer instance\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Fit and transform the training data\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "    # Transform the test data using the same vectorizer\n",
        "    X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train SVC classifier\n",
        "    clf = SVC()\n",
        "    clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    return accuracy, precision, recall\n",
        "\n",
        "# Repeat the process 10 times\n",
        "num_repeats = 10\n",
        "accuracies_svc = []\n",
        "precisions_svc = []\n",
        "recalls_svc = []\n",
        "\n",
        "for i in range(num_repeats):\n",
        "    # Load and shuffle reviews\n",
        "    X, y = load_and_shuffle_reviews()\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "\n",
        "    # Train and evaluate the SVC classifier\n",
        "    accuracy_svc, precision_svc, recall_svc = train_and_evaluate_svc(X_train, X_test, y_train, y_test)\n",
        "    accuracies_svc.append(accuracy_svc)\n",
        "    precisions_svc.append(precision_svc)\n",
        "    recalls_svc.append(recall_svc)\n",
        "\n",
        "    print(f'Repeat {i+1}: Accuracy (SVC) = {accuracy_svc:.2f}, Precision = {precision_svc:.2f}, Recall = {recall_svc:.2f}')\n",
        "\n",
        "# Display the average accuracy, precision, and recall over 10 repeats for SVC\n",
        "average_accuracy_svc = np.mean(accuracies_svc)\n",
        "average_precision_svc = np.mean(precisions_svc)\n",
        "average_recall_svc = np.mean(recalls_svc)\n",
        "\n",
        "print(f'\\nAverage Accuracy (SVC) over {num_repeats} repeats: {average_accuracy_svc:.2f}')\n",
        "print(f'Average Precision (SVC) over {num_repeats} repeats: {average_precision_svc:.2f}')\n",
        "print(f'Average Recall (SVC) over {num_repeats} repeats: {average_recall_svc:.2f}')\n"
      ],
      "metadata": {
        "id": "FbJUsh_GPlmF",
        "outputId": "3e48cb57-acb2-48da-c809-6f3e9fa5a248",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repeat 1: Accuracy (SVC) = 0.86, Precision = 0.86, Recall = 0.86\n",
            "Repeat 2: Accuracy (SVC) = 0.86, Precision = 0.86, Recall = 0.86\n",
            "Repeat 3: Accuracy (SVC) = 0.86, Precision = 0.86, Recall = 0.86\n",
            "Repeat 4: Accuracy (SVC) = 0.80, Precision = 0.80, Recall = 0.80\n",
            "Repeat 5: Accuracy (SVC) = 0.83, Precision = 0.83, Recall = 0.83\n",
            "Repeat 6: Accuracy (SVC) = 0.82, Precision = 0.82, Recall = 0.82\n",
            "Repeat 7: Accuracy (SVC) = 0.81, Precision = 0.82, Recall = 0.81\n",
            "Repeat 8: Accuracy (SVC) = 0.81, Precision = 0.81, Recall = 0.81\n",
            "Repeat 9: Accuracy (SVC) = 0.83, Precision = 0.84, Recall = 0.83\n",
            "Repeat 10: Accuracy (SVC) = 0.84, Precision = 0.85, Recall = 0.84\n",
            "\n",
            "Average Accuracy (SVC) over 10 repeats: 0.83\n",
            "Average Precision (SVC) over 10 repeats: 0.83\n",
            "Average Recall (SVC) over 10 repeats: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results, SVC was more accurate, precise, and recalled better than Bernoullis. What this means is that it likely predicted whether a review was correctly positive or negative more often and avoided false positives and negatives over the span of 10 repeats. This is likely for SVM being more complex than Naive Bayes which is simpler and computes faster. SVC has the advantage of being used for more than just binary data and can go higher dimensionality but the higher computing costs and time are something to look at and for this data the small difference shows that maybe the improved accuracy may not be worth it."
      ],
      "metadata": {
        "id": "M-dPAnieRVB5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "text_classification_assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "94px",
        "width": "252px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}